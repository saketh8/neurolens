# NeuroLens ğŸ‘ï¸

**Real-Time Accessibility AI Assistant for Visually Impaired Users**

NeuroLens is a privacy-first mobile application that empowers visually impaired individuals with a **Hybrid Intelligence Architecture**. It combines instant, on-device object detection (Arm-optimized) with deep scene understanding (Mistral AI) to deliver both safety and context.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Platform](https://img.shields.io/badge/platform-iOS%20%7C%20Android-lightgrey.svg)
![Arm Optimized](https://img.shields.io/badge/Arm-Optimized-green.svg)

## ğŸŒŸ Features

### ğŸ¯ Core Capabilities

-   **Reflex Mode (Local)**: Instant object detection (<100ms) for obstacle avoidance using TensorFlow.js on Arm.
-   **Cognitive Mode (Cloud)**: Detailed scene description and Q&A using Mistral AI.
-   **Spatial Awareness**: Haptic feedback for object proximity.
-   **Voice-First Interface**: Fully controllable through voice commands.
-   **Hybrid Privacy**: Continuous video processing is local; only specific images are sent to cloud on demand.

### ğŸ¨ Three Operating Modes

1.  **Reflex Mode** ğŸ‘ï¸ - Instant safety warnings and object ID (Offline).
2.  **Cognitive Mode** ğŸ§  - "What do you see?" detailed descriptions (Online).
3.  **Navigation Mode** ğŸ§­ - Directional guidance to specific objects.

## ğŸ—ï¸ Architecture

NeuroLens leverages a **Hybrid Architecture** optimized for Arm:

-   **Local Layer**: TensorFlow.js (SSD MobileNet v2) running on Arm GPU via WebGL.
-   **Cloud Layer**: Mistral AI (Vision) for complex reasoning.
-   **Expo Camera**: Real-time video processing.
-   **Native TTS/STT**: Voice interface.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Camera Input                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Local  â”‚      â”‚  Cloud   â”‚
â”‚ Reflex â”‚      â”‚Cognitive â”‚
â”‚ (TFJS) â”‚      â”‚(Mistral) â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
        â”‚  Output  â”‚
        â”‚ (Voice/  â”‚
        â”‚ Haptics) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Getting Started

### Prerequisites

-   Node.js 20+
-   Expo Go app installed on your device

### Installation

1.  **Clone the repository**
    ```bash
    git clone https://github.com/saketh8/neurolens.git
    cd neurolens
    ```

2.  **Install dependencies**
    ```bash
    npm install
    ```

3.  **Start the development server**
    ```bash
    npx expo start --clear
    ```

4.  **Run on your device**
    -   Scan the QR code with Expo Go (Android) or Camera (iOS).

## ğŸ“± Usage

### Basic Operation

1.  **Launch NeuroLens** - Grant camera permissions.
2.  **Reflex Mode (Default)** - Point at objects to hear them instantly. Works offline.
3.  **Cognitive Mode** - Tap the screen to ask for a detailed description.

### Voice Commands

-   "What do you see?" - Triggers Cognitive Mode description.
-   "Find exit" - Switches to Navigation Mode.

## ğŸ› ï¸ Development

### Project Structure

```
NeuroLens/
â”œâ”€â”€ App.tsx                 # Main app entry
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ screens/
â”‚   â”‚   â””â”€â”€ CameraScreen.tsx    # Main camera interface
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ VisionService.ts    # Local TFJS Object Detection
â”‚   â”‚   â”œâ”€â”€ MistralService.ts   # Cloud Scene Description
â”‚   â”‚   â”œâ”€â”€ VoiceService.ts     # TTS/STT
â”‚   â”‚   â””â”€â”€ HapticService.ts    # Haptic feedback
â”‚   â””â”€â”€ utils/
â””â”€â”€ package.json
```

## ğŸ¯ Arm Optimization

NeuroLens is specifically optimized for Arm architecture:

-   **WebGL Acceleration** - Uses `tfjs-react-native` to access the Arm GPU.
-   **Quantized Models** - Uses efficient MobileNet v2 for low latency.
-   **Hybrid Offloading** - Only heavy tasks go to the cloud, saving battery.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

-   Built for the Arm AI Developer Challenge.
-   Powered by TensorFlow.js and Mistral AI.
