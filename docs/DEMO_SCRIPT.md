# NeuroLens - Demo Video Script

**Duration**: 2:30 minutes  
**Target**: Arm AI Developer Challenge Judges

---

## Scene 1: Opening (0:00 - 0:20)

**Visual**: NeuroLens logo with tagline  
**Voiceover**: 
> "285 million people worldwide are visually impaired. They face daily challenges we often take for granted—reading signs, navigating unfamiliar places, identifying objects. Meet NeuroLens: an AI-powered assistant that runs entirely on your phone, completely offline, keeping your privacy intact."

**Text Overlay**: 
- "NeuroLens"
- "Privacy-First AI Accessibility"
- "Powered by Arm"

---

## Scene 2: The Problem (0:20 - 0:35)

**Visual**: Split screen showing common challenges
- Person struggling to read a menu
- Difficulty identifying objects
- Navigation challenges

**Voiceover**:
> "Current solutions require internet connectivity, send your data to the cloud, or lack real-time responsiveness. NeuroLens solves all of these problems by running sophisticated AI models directly on Arm-based devices."

---

## Scene 3: Scene Description Mode (0:35 - 1:00)

**Visual**: Phone camera view, person pointing at a room

**Demo Actions**:
1. Launch app
2. Tap screen
3. App speaks: "You're in an indoor area with moderate lighting. Close to you: chair, table. A few meters away: couch, lamp."

**Voiceover**:
> "Scene mode uses real-time object detection to describe your surroundings. Powered by TensorFlow Lite optimized for Arm, it identifies objects and estimates distances in under 100 milliseconds."

**Text Overlay**:
- "Object Detection: <100ms"
- "30fps Camera Processing"
- "Arm NEON Optimized"

---

## Scene 4: Text Reading Mode (1:00 - 1:20)

**Visual**: Phone camera pointing at a sign/menu

**Demo Actions**:
1. Switch to text mode (tap middle button)
2. Point at restaurant menu
3. App speaks: "Menu. Appetizers: Spring Rolls $8, Soup $6..."

**Voiceover**:
> "Text mode uses OCR to read signs, menus, documents, even currency. The text is organized naturally, making it easy to understand."

**Text Overlay**:
- "OCR Processing: <200ms"
- "Multi-language Support"
- "Currency Recognition"

---

## Scene 5: Navigation Mode (1:20 - 1:40)

**Visual**: Person using app to navigate

**Demo Actions**:
1. Switch to navigation mode
2. Say "Find exit sign"
3. App provides directional haptic feedback
4. App speaks: "Exit sign 3 meters ahead, slightly to your right"

**Voiceover**:
> "Navigation mode combines vision AI with haptic feedback to guide users. Different vibration patterns indicate direction and distance."

**Text Overlay**:
- "Voice Commands"
- "Haptic Guidance"
- "Spatial Awareness"

---

## Scene 6: Privacy & Technology (1:40 - 2:00)

**Visual**: Animation showing on-device processing

**Voiceover**:
> "Everything runs on-device using ExecuTorch with a quantized Llama model. Your camera feed never leaves your phone. No internet required. Complete privacy guaranteed."

**Text Overlay**:
- "100% On-Device Processing"
- "ExecuTorch + TensorFlow Lite"
- "Zero Cloud Dependency"
- "LLM Inference: <2s"

**Visual**: Architecture diagram showing:
```
Camera → Vision AI → LLM → Voice Output
              ↓
         Haptic Feedback
```

---

## Scene 7: Impact & Open Source (2:00 - 2:20)

**Visual**: Code snippets, GitHub repository

**Voiceover**:
> "NeuroLens is fully open-source. Our architecture demonstrates best practices for on-device AI on Arm platforms. The modular services can be reused in other accessibility projects."

**Text Overlay**:
- "MIT Licensed"
- "Open Source"
- "Reusable Components"
- "Arm Optimization Reference"

**Visual**: Show GitHub stars, forks, community

---

## Scene 8: Closing (2:20 - 2:30)

**Visual**: Person using NeuroLens successfully navigating, smiling

**Voiceover**:
> "NeuroLens: Empowering independence through privacy-first AI. Built for accessibility. Optimized for Arm. Open for everyone."

**Text Overlay**:
- "NeuroLens"
- "github.com/yourusername/neurolens"
- "Built for Arm AI Developer Challenge"

**Final Frame**: 
- NeuroLens logo
- QR code to GitHub repo
- "Made with ❤️ for accessibility"

---

## Recording Tips

### Equipment
- Use actual Arm-based device (iPhone or Android)
- Good lighting for screen recording
- Clear audio for voiceover
- Screen recording software (QuickTime, AZ Screen Recorder)

### Filming Locations
1. **Indoor scene**: Living room or office with various objects
2. **Text reading**: Restaurant menu, street sign, or book
3. **Navigation**: Hallway or outdoor area with clear landmarks

### Post-Production
- Add text overlays at specified timestamps
- Include background music (royalty-free, subtle)
- Color grade for consistency
- Add captions for accessibility
- Export at 1080p, 30fps

### Key Messages to Emphasize
1. **Privacy**: On-device processing, no cloud
2. **Performance**: Real-time, optimized for Arm
3. **Impact**: Helps 285M people worldwide
4. **Innovation**: Multiple AI models running simultaneously
5. **Open Source**: Community contribution potential

---

## B-Roll Suggestions

- Close-up of haptic feedback (hand feeling vibrations)
- Split screen: with/without NeuroLens
- Performance metrics graphs
- Code snippets showing Arm optimizations
- User testimonials (if available)
- Comparison with cloud-based solutions (latency)

---

## Call to Action

**End Screen**:
- "Try NeuroLens Today"
- "Star on GitHub"
- "Contribute to Accessibility"
- "Built with Arm ExecuTorch & TensorFlow Lite"

---

## Alternative 60-Second Version

For social media or quick demos:

1. **0:00-0:10**: Problem statement
2. **0:10-0:25**: Scene mode demo
3. **0:25-0:40**: Text mode demo
4. **0:40-0:50**: Privacy & on-device AI
5. **0:50-1:00**: Call to action

---

**Remember**: The demo should evoke emotion. Show real impact on real people. Make judges think "This could change lives."
